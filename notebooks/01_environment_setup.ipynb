{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9143f8c6",
   "metadata": {},
   "source": [
    "# AI vs Human Writing Classification - Environment Setup\n",
    "\n",
    "This notebook will help you set up the complete environment for the AI vs Human Writing Classification project. We'll install all necessary libraries, configure the project structure, and test everything works correctly.\n",
    "\n",
    "## Project Overview\n",
    "This project aims to classify text as either AI-generated or human-written using various machine learning approaches including traditional ML models and transformer-based deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b8515b",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "First, let's install all the necessary Python packages. This includes machine learning libraries, NLP tools, and data processing utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce52e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core ML and data science libraries\n",
    "!pip install numpy>=1.24.0 pandas>=2.0.0 scikit-learn>=1.3.0 scipy>=1.10.0\n",
    "\n",
    "# Install deep learning frameworks\n",
    "!pip install torch>=2.0.0 transformers>=4.30.0 datasets>=2.12.0\n",
    "\n",
    "# Install NLP libraries\n",
    "!pip install nltk>=3.8 spacy>=3.6.0 textblob>=0.17.1 gensim>=4.3.0\n",
    "\n",
    "# Install visualization libraries\n",
    "!pip install matplotlib>=3.7.0 seaborn>=0.12.0 plotly>=5.14.0\n",
    "\n",
    "# Install utility libraries\n",
    "!pip install tqdm>=4.65.0 click>=8.1.0 pyyaml>=6.0 python-dotenv>=1.0.0\n",
    "\n",
    "# Install additional text processing tools\n",
    "!pip install wordcloud>=1.9.0 textstat>=0.7.0\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ac937",
   "metadata": {},
   "source": [
    "## 2. Import Essential Libraries\n",
    "\n",
    "Now let's import all the essential libraries and verify they work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "import textstat\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Utility libraries\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ba372",
   "metadata": {},
   "source": [
    "## 3. Set Up Project Directory Structure\n",
    "\n",
    "Let's verify and organize our project directory structure for better organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5806ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project root\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Define directory structure\n",
    "directories = {\n",
    "    'data': PROJECT_ROOT / 'data',\n",
    "    'data_raw': PROJECT_ROOT / 'data' / 'raw',\n",
    "    'data_processed': PROJECT_ROOT / 'data' / 'processed',\n",
    "    'models': PROJECT_ROOT / 'models',\n",
    "    'models_trained': PROJECT_ROOT / 'models' / 'trained',\n",
    "    'models_checkpoints': PROJECT_ROOT / 'models' / 'checkpoints',\n",
    "    'results': PROJECT_ROOT / 'results',\n",
    "    'config': PROJECT_ROOT / 'config',\n",
    "    'notebooks': PROJECT_ROOT / 'notebooks',\n",
    "    'src': PROJECT_ROOT / 'src'\n",
    "}\n",
    "\n",
    "# Verify all directories exist\n",
    "print(\"\\nüìÅ Directory Structure:\")\n",
    "print(\"=\" * 40)\n",
    "for name, path in directories.items():\n",
    "    exists = \"‚úÖ\" if path.exists() else \"‚ùå\"\n",
    "    print(f\"{exists} {name:20}: {path}\")\n",
    "\n",
    "# Add src to Python path for imports\n",
    "src_path = str(PROJECT_ROOT / 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "    print(f\"\\n‚úÖ Added {src_path} to Python path\")\n",
    "\n",
    "print(\"\\n‚úÖ Project structure verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b35b7c",
   "metadata": {},
   "source": [
    "## 4. Configure Environment Variables\n",
    "\n",
    "Let's set up configuration variables and load the project configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bcbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load project configuration\n",
    "config_path = PROJECT_ROOT / 'config' / 'config.yaml'\n",
    "\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    print(\"‚úÖ Configuration loaded successfully!\")\n",
    "    print(\"\\nüìã Configuration Overview:\")\n",
    "    print(\"=\" * 40)\n",
    "    for section, settings in config.items():\n",
    "        print(f\"üìÇ {section.upper()}:\")\n",
    "        if isinstance(settings, dict):\n",
    "            for key, value in settings.items():\n",
    "                print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ {settings}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Configuration file not found. Using default settings.\")\n",
    "    config = {\n",
    "        'model': {\n",
    "            'name': 'bert-base-uncased',\n",
    "            'max_length': 512,\n",
    "            'num_labels': 2\n",
    "        },\n",
    "        'data': {\n",
    "            'text_column': 'text',\n",
    "            'label_column': 'label',\n",
    "            'min_text_length': 50\n",
    "        },\n",
    "        'training': {\n",
    "            'batch_size': 16,\n",
    "            'epochs': 3,\n",
    "            'test_split': 0.2\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid tokenizer warnings\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(PROJECT_ROOT / 'models' / 'cache')\n",
    "\n",
    "print(\"\\n‚úÖ Environment variables configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7d9e0d",
   "metadata": {},
   "source": [
    "## 5. Download NLTK Data and Test NLP Tools\n",
    "\n",
    "Let's download necessary NLTK data and test our NLP tools with sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk_downloads = ['punkt', 'averaged_perceptron_tagger', 'stopwords']\n",
    "\n",
    "print(\"üì• Downloading NLTK data...\")\n",
    "for item in nltk_downloads:\n",
    "    try:\n",
    "        nltk.download(item, quiet=True)\n",
    "        print(f\"‚úÖ Downloaded: {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to download {item}: {e}\")\n",
    "\n",
    "# Test with sample texts\n",
    "sample_texts = {\n",
    "    'human': \"\"\"\n",
    "    The old oak tree stood majestically in the center of the park, its gnarled branches \n",
    "    reaching toward the cloudy sky. Children often played beneath its shade during summer \n",
    "    afternoons, their laughter echoing through the leaves. I remember spending countless \n",
    "    hours there as a child, reading books and watching the world go by.\n",
    "    \"\"\",\n",
    "    'ai': \"\"\"\n",
    "    Machine learning algorithms have revolutionized the field of artificial intelligence \n",
    "    by enabling computers to learn patterns from data without explicit programming. \n",
    "    These algorithms can be categorized into supervised, unsupervised, and reinforcement \n",
    "    learning approaches, each with specific applications and methodologies.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(\"\\nüß™ Testing NLP Tools:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for label, text in sample_texts.items():\n",
    "    text = text.strip()\n",
    "    print(f\"\\nüìù {label.upper()} TEXT ANALYSIS:\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    word_count = len(text.split())\n",
    "    char_count = len(text)\n",
    "    sentence_count = len(nltk.sent_tokenize(text))\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Words: {word_count}\")\n",
    "    print(f\"   ‚Ä¢ Characters: {char_count}\")\n",
    "    print(f\"   ‚Ä¢ Sentences: {sentence_count}\")\n",
    "    \n",
    "    # Readability scores\n",
    "    try:\n",
    "        flesch_score = textstat.flesch_reading_ease(text)\n",
    "        grade_level = textstat.flesch_kincaid_grade(text)\n",
    "        print(f\"   ‚Ä¢ Flesch Reading Ease: {flesch_score:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Grade Level: {grade_level:.2f}\")\n",
    "    except:\n",
    "        print(\"   ‚Ä¢ Readability scores: Could not compute\")\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    try:\n",
    "        blob = TextBlob(text)\n",
    "        sentiment = blob.sentiment\n",
    "        print(f\"   ‚Ä¢ Sentiment Polarity: {sentiment.polarity:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Sentiment Subjectivity: {sentiment.subjectivity:.2f}\")\n",
    "    except:\n",
    "        print(\"   ‚Ä¢ Sentiment: Could not compute\")\n",
    "\n",
    "print(\"\\n‚úÖ NLP tools are working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776b88f",
   "metadata": {},
   "source": [
    "## 6. Test Environment Setup\n",
    "\n",
    "Let's run comprehensive tests to verify everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a072b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Test our custom modules\n",
    "print(\"üß™ Testing Custom Modules:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Test data preprocessing\n",
    "    from data import TextPreprocessor\n",
    "    preprocessor = TextPreprocessor()\n",
    "    test_text = \"This is a   test text with   extra spaces!!!\"\n",
    "    cleaned = preprocessor.clean_text(test_text)\n",
    "    print(f\"‚úÖ Text preprocessing: '{test_text}' ‚Üí '{cleaned}'\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ÑπÔ∏è  Custom data module not yet available: {e}\")\n",
    "\n",
    "try:\n",
    "    # Test feature extraction\n",
    "    from data.features import LinguisticFeatureExtractor\n",
    "    extractor = LinguisticFeatureExtractor()\n",
    "    features = extractor.extract_basic_stats(sample_texts['human'])\n",
    "    print(f\"‚úÖ Feature extraction: Extracted {len(features)} features\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ÑπÔ∏è  Custom feature module not yet available: {e}\")\n",
    "\n",
    "# Test 2: Test machine learning pipeline\n",
    "print(\"\\nü§ñ Testing ML Pipeline:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create sample dataset\n",
    "sample_data = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"I love walking in the park during autumn.\",\n",
    "        \"The algorithm processes data efficiently using neural networks.\",\n",
    "        \"My grandmother makes the best apple pie in the world.\",\n",
    "        \"Machine learning models require large datasets for training.\",\n",
    "        \"The sunset painted the sky in beautiful orange hues.\",\n",
    "        \"Deep learning architectures utilize multiple hidden layers.\"\n",
    "    ],\n",
    "    'label': [0, 1, 0, 1, 0, 1]  # 0 = human, 1 = AI\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Created sample dataset with {len(sample_data)} samples\")\n",
    "\n",
    "# Test TF-IDF vectorization\n",
    "try:\n",
    "    vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "    X = vectorizer.fit_transform(sample_data['text'])\n",
    "    y = sample_data['label']\n",
    "    print(f\"‚úÖ TF-IDF vectorization: Shape {X.shape}\")\n",
    "    \n",
    "    # Test train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    print(f\"‚úÖ Train-test split: Train {X_train.shape[0]}, Test {X_test.shape[0]}\")\n",
    "    \n",
    "    # Test model training\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"‚úÖ Model training: Accuracy {accuracy:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ML pipeline test failed: {e}\")\n",
    "\n",
    "# Test 3: Test transformer model loading\n",
    "print(\"\\nü§ó Testing Transformer Models:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    model_name = config['model']['name']\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Test tokenization\n",
    "    test_text = \"This is a test sentence for tokenization.\"\n",
    "    tokens = tokenizer(test_text, return_tensors='pt', padding=True, truncation=True)\n",
    "    print(f\"‚úÖ Tokenizer loaded: {model_name}\")\n",
    "    print(f\"‚úÖ Tokenization test: {len(tokens['input_ids'][0])} tokens\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Transformer test failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üéâ ENVIRONMENT SETUP COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n‚úÖ All core components are working correctly!\")\n",
    "print(\"‚úÖ You're ready to start building your AI vs Human text classifier!\")\n",
    "print(\"\\nüìù Next steps:\")\n",
    "print(\"   1. Prepare your dataset (CSV with 'text' and 'label' columns)\")\n",
    "print(\"   2. Use the training script: python train.py --data your_data.csv\")\n",
    "print(\"   3. Explore the notebooks/ directory for examples\")\n",
    "print(\"   4. Check the src/ directory for the main code modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b290980c",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "Your AI vs Human Writing Classification environment is now fully set up! Here's what we've accomplished:\n",
    "\n",
    "### ‚úÖ Environment Setup Complete\n",
    "- **Libraries Installed**: All necessary ML, NLP, and data processing libraries\n",
    "- **Project Structure**: Organized directory structure for data, models, and results\n",
    "- **Configuration**: YAML-based configuration system\n",
    "- **NLTK Data**: Downloaded necessary language processing data\n",
    "- **Testing**: Verified all components work correctly\n",
    "\n",
    "### üöÄ Ready for Development\n",
    "Your environment includes:\n",
    "- **Traditional ML Models**: Logistic Regression, Random Forest, SVM\n",
    "- **Deep Learning**: BERT and transformer-based models\n",
    "- **Feature Engineering**: Linguistic and stylometric features\n",
    "- **Evaluation Tools**: Comprehensive model assessment utilities\n",
    "\n",
    "### üìÅ Project Structure\n",
    "```\n",
    "AI-vs-Human-Writing-Classifier/\n",
    "‚îú‚îÄ‚îÄ data/                   # Your datasets\n",
    "‚îú‚îÄ‚îÄ models/                 # Saved models\n",
    "‚îú‚îÄ‚îÄ notebooks/              # Jupyter notebooks\n",
    "‚îú‚îÄ‚îÄ src/                    # Source code modules\n",
    "‚îú‚îÄ‚îÄ config/                 # Configuration files\n",
    "‚îú‚îÄ‚îÄ results/                # Experiment results\n",
    "‚îú‚îÄ‚îÄ requirements.txt        # Dependencies\n",
    "‚îî‚îÄ‚îÄ train.py               # Training script\n",
    "```\n",
    "\n",
    "### üé® Next Steps\n",
    "1. **Data Collection**: Gather or download AI-generated and human-written text datasets\n",
    "2. **Data Exploration**: Use notebooks to explore your data\n",
    "3. **Model Training**: Run experiments with different models\n",
    "4. **Evaluation**: Compare model performance and analyze results\n",
    "\n",
    "Happy coding! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
