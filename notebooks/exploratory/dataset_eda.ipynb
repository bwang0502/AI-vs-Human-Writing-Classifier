{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c8940b",
   "metadata": {},
   "source": [
    "# AI vs Human Content Detection - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on the AI vs Human content detection dataset.\n",
    "\n",
    "## Dataset Overview\n",
    "- **File**: `data/raw/ai_human_content_detection_dataset.csv`\n",
    "- **Task**: Binary classification (AI-generated vs Human-written)\n",
    "- **Features**: Text content + multiple linguistic/stylometric features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887575b8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df0c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add src to path for our custom modules\n",
    "sys.path.append('../../src')\n",
    "\n",
    "# Set style and suppress warnings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75768a91",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c728129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../../data/raw/ai_human_content_detection_dataset.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25825750",
   "metadata": {},
   "source": [
    "## 3. Basic Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1600c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Number of samples: {len(df):,}\")\n",
    "print(f\"Number of features: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== COLUMN INFORMATION ===\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few rows\n",
    "print(\"=== FIRST 5 ROWS ===\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec59330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"=== STATISTICAL SUMMARY ===\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e5073",
   "metadata": {},
   "source": [
    "## 4. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percent\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(\"=== MISSING VALUES ===\")\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "if missing_df['Missing Count'].sum() == 0:\n",
    "    print(\"✅ No missing values found in the dataset!\")\n",
    "else:\n",
    "    # Visualize missing values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_cols = missing_df[missing_df['Missing Count'] > 0]\n",
    "    if len(missing_cols) > 0:\n",
    "        plt.bar(missing_cols.index, missing_cols['Missing Percentage'])\n",
    "        plt.title('Missing Values by Column')\n",
    "        plt.xlabel('Columns')\n",
    "        plt.ylabel('Missing Percentage (%)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d3944",
   "metadata": {},
   "source": [
    "## 5. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ade3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the target variable (label)\n",
    "print(\"=== TARGET VARIABLE ANALYSIS ===\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "label_counts = df['label'].value_counts().sort_index()\n",
    "print(label_counts)\n",
    "\n",
    "print(\"\\nLabel percentages:\")\n",
    "label_percentages = df['label'].value_counts(normalize=True).sort_index() * 100\n",
    "for label, pct in label_percentages.items():\n",
    "    label_name = \"Human\" if label == 0 else \"AI-Generated\"\n",
    "    print(f\"{label_name} ({label}): {pct:.2f}%\")\n",
    "\n",
    "# Visualize label distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bar plot\n",
    "labels = ['Human (0)', 'AI-Generated (1)']\n",
    "colors = ['skyblue', 'lightcoral']\n",
    "ax1.bar(labels, label_counts.values, color=colors)\n",
    "ax1.set_title('Distribution of Labels')\n",
    "ax1.set_ylabel('Count')\n",
    "for i, v in enumerate(label_counts.values):\n",
    "    ax1.text(i, v + 100, str(v), ha='center', va='bottom')\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(label_counts.values, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax2.set_title('Label Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if dataset is balanced\n",
    "balance_ratio = min(label_counts) / max(label_counts)\n",
    "print(f\"\\nDataset balance ratio: {balance_ratio:.3f}\")\n",
    "if balance_ratio > 0.8:\n",
    "    print(\"✅ Dataset is well balanced\")\n",
    "elif balance_ratio > 0.5:\n",
    "    print(\"⚠️ Dataset is moderately imbalanced\")\n",
    "else:\n",
    "    print(\"❌ Dataset is highly imbalanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f08dd",
   "metadata": {},
   "source": [
    "## 6. Text Content Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text content lengths\n",
    "print(\"=== TEXT CONTENT ANALYSIS ===\")\n",
    "\n",
    "# Calculate text lengths\n",
    "df['text_length'] = df['text_content'].str.len()\n",
    "\n",
    "print(\"\\nText length statistics:\")\n",
    "print(df.groupby('label')['text_length'].describe())\n",
    "\n",
    "# Visualize text length distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Overall text length distribution\n",
    "axes[0, 0].hist(df['text_length'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Overall Text Length Distribution')\n",
    "axes[0, 0].set_xlabel('Text Length (characters)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Text length by label\n",
    "for label in df['label'].unique():\n",
    "    label_name = \"Human\" if label == 0 else \"AI-Generated\"\n",
    "    data = df[df['label'] == label]['text_length']\n",
    "    axes[0, 1].hist(data, bins=30, alpha=0.6, label=label_name)\n",
    "axes[0, 1].set_title('Text Length Distribution by Label')\n",
    "axes[0, 1].set_xlabel('Text Length (characters)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Box plot comparison\n",
    "df.boxplot(column='text_length', by='label', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Text Length by Label (Box Plot)')\n",
    "axes[1, 0].set_xlabel('Label')\n",
    "axes[1, 0].set_ylabel('Text Length')\n",
    "\n",
    "# Word count vs character count\n",
    "scatter = axes[1, 1].scatter(df['word_count'], df['character_count'], \n",
    "                           c=df['label'], alpha=0.6, cmap='coolwarm')\n",
    "axes[1, 1].set_title('Word Count vs Character Count')\n",
    "axes[1, 1].set_xlabel('Word Count')\n",
    "axes[1, 1].set_ylabel('Character Count')\n",
    "plt.colorbar(scatter, ax=axes[1, 1], label='Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916c063",
   "metadata": {},
   "source": [
    "## 7. Linguistic Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2330778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze linguistic features\n",
    "print(\"=== LINGUISTIC FEATURES ANALYSIS ===\")\n",
    "\n",
    "# List of numeric features to analyze\n",
    "linguistic_features = [\n",
    "    'word_count', 'character_count', 'sentence_count', 'lexical_diversity',\n",
    "    'avg_sentence_length', 'avg_word_length', 'punctuation_ratio',\n",
    "    'flesch_reading_ease', 'gunning_fog_index', 'grammar_errors',\n",
    "    'passive_voice_ratio', 'predictability_score', 'burstiness', 'sentiment_score'\n",
    "]\n",
    "\n",
    "# Statistical comparison by label\n",
    "print(\"\\nFeature comparison by label (mean values):\")\n",
    "feature_comparison = df.groupby('label')[linguistic_features].mean()\n",
    "print(feature_comparison.round(3))\n",
    "\n",
    "# Calculate differences\n",
    "print(\"\\nDifference (AI - Human):\")\n",
    "if 1 in feature_comparison.index and 0 in feature_comparison.index:\n",
    "    differences = feature_comparison.loc[1] - feature_comparison.loc[0]\n",
    "    print(differences.round(3))\n",
    "\n",
    "# Visualize feature distributions\n",
    "n_features = len(linguistic_features)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "\n",
    "for i, feature in enumerate(linguistic_features):\n",
    "    if i < len(axes):\n",
    "        for label in df['label'].unique():\n",
    "            label_name = \"Human\" if label == 0 else \"AI-Generated\"\n",
    "            data = df[df['label'] == label][feature]\n",
    "            axes[i].hist(data, bins=30, alpha=0.6, label=label_name)\n",
    "        \n",
    "        axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].legend()\n",
    "\n",
    "# Hide extra subplots\n",
    "for i in range(len(linguistic_features), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa15520c",
   "metadata": {},
   "source": [
    "## 8. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1716d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"=== FEATURE CORRELATION ANALYSIS ===\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_features = linguistic_features + ['label']\n",
    "correlation_matrix = df[corr_features].corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Features most correlated with label\n",
    "print(\"\\nFeatures most correlated with label:\")\n",
    "label_correlations = correlation_matrix['label'].abs().sort_values(ascending=False)\n",
    "print(label_correlations[label_correlations.index != 'label'])\n",
    "\n",
    "# Plot top correlated features\n",
    "top_features = label_correlations[label_correlations.index != 'label'].head(8)\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red' if x < 0 else 'blue' for x in correlation_matrix['label'][top_features.index]]\n",
    "plt.barh(range(len(top_features)), top_features.values, color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), [f.replace('_', ' ').title() for f in top_features.index])\n",
    "plt.xlabel('Absolute Correlation with Label')\n",
    "plt.title('Top Features Correlated with Label')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ec7f9",
   "metadata": {},
   "source": [
    "## 9. Sample Text Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b868ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sample texts between AI and human\n",
    "print(\"=== SAMPLE TEXT COMPARISON ===\")\n",
    "\n",
    "# Get samples from each class\n",
    "human_samples = df[df['label'] == 0]['text_content'].head(3)\n",
    "ai_samples = df[df['label'] == 1]['text_content'].head(3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HUMAN-WRITTEN SAMPLES\")\n",
    "print(\"=\"*60)\n",
    "for i, text in enumerate(human_samples, 1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(text[:500] + (\"...\" if len(text) > 500 else \"\"))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AI-GENERATED SAMPLES\")\n",
    "print(\"=\"*60)\n",
    "for i, text in enumerate(ai_samples, 1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(text[:500] + (\"...\" if len(text) > 500 else \"\"))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f54eb97",
   "metadata": {},
   "source": [
    "## 10. Feature Importance for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick feature importance analysis using Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "\n",
    "# Prepare data\n",
    "X = df[linguistic_features].fillna(0)\n",
    "y = df['label']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_scaled, y)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': linguistic_features,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Random Forest):\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(feature_importance)), feature_importance['importance'])\n",
    "plt.yticks(range(len(feature_importance)), \n",
    "           [f.replace('_', ' ').title() for f in feature_importance['feature']])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance for AI vs Human Classification')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d3845",
   "metadata": {},
   "source": [
    "## 11. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bac45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATASET SUMMARY AND INSIGHTS ===\")\n",
    "print(f\"\"\"    \n",
    "📊 DATASET OVERVIEW:\n",
    "• Total samples: {len(df):,}\n",
    "• Features: {len(linguistic_features)} linguistic features + text content\n",
    "• Target: Binary classification (Human=0, AI=1)\n",
    "• Balance: {label_percentages[0]:.1f}% Human, {label_percentages[1]:.1f}% AI\n",
    "\n",
    "📈 KEY FINDINGS:\n",
    "• Most discriminative features: {', '.join(top_features.head(3).index)}\n",
    "• Average text length: {df['text_length'].mean():.0f} characters\n",
    "• Text length range: {df['text_length'].min()} - {df['text_length'].max()} characters\n",
    "\n",
    "🔍 INSIGHTS FOR MODELING:\n",
    "• Dataset appears well-balanced for binary classification\n",
    "• Multiple linguistic features show correlation with AI/Human labels\n",
    "• Text length and complexity metrics may be strong predictors\n",
    "• Ready for feature engineering and model training\n",
    "\n",
    "✅ NEXT STEPS:\n",
    "1. Feature engineering and selection\n",
    "2. Train baseline models (Logistic Regression, Random Forest)\n",
    "3. Experiment with advanced models (BERT, transformers)\n",
    "4. Cross-validation and hyperparameter tuning\n",
    "5. Model interpretation and error analysis\n",
    "\"\"\")\n",
    "\n",
    "# Save key statistics\n",
    "summary_stats = {\n",
    "    'total_samples': len(df),\n",
    "    'human_samples': len(df[df['label'] == 0]),\n",
    "    'ai_samples': len(df[df['label'] == 1]),\n",
    "    'balance_ratio': balance_ratio,\n",
    "    'avg_text_length': df['text_length'].mean(),\n",
    "    'top_correlated_features': top_features.head(5).index.tolist(),\n",
    "    'most_important_features': feature_importance.head(5)['feature'].tolist()\n",
    "}\n",
    "\n",
    "print(\"\\n📁 Summary statistics saved for future reference.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
