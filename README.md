# AI vs Human Text Classifier

Two transformer-based models for detecting AI-generated text:
- **Binary Classifier**: Human vs AI (94% accuracy)
- **Multi-Class Classifier**: Identify which AI model wrote the text (82% F1-score)

## üöÄ Quick Start

1. Clone the repo:
```bash
git clone https://github.com/bwang0502/AI-vs-Human-Writing-Classifier.git
cd AI-vs-Human-Writing-Classifier
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. **Download pre-trained models** (too large for GitHub):
   - Download from [Google Drive link - ADD YOUR LINK]
   - Extract to `results/` folder

4. Run Streamlit app:
```bash
streamlit run app.py
```

## üìä Model Performance

### Binary Classifier
- Accuracy: 94%
- F1-Score: 94%

### Multi-Class Classifier (7 classes)
- Overall F1: 82%
- Human detection: 96% F1
- Claude detection: 90% F1

## üóÇÔ∏è Project Structure

````markdown
# AI vs Human Text Detector

A Streamlit web application that uses a fine-tuned DistilBERT transformer model to detect whether text was written by a human or generated by AI.

## Features

- **Binary Classification**: Distinguishes between human-written and AI-generated text
- **Confidence Scores**: Provides probability distributions for predictions
- **LIME Explanations**: Shows which words influenced the model's decision
- **Interactive UI**: Easy-to-use interface with example texts

## Model

- **Architecture**: DistilBERT (fine-tuned)
- **Training Data**: 15,000 text samples
- **Performance**: 
  - F1-Score: [Your score from checkpoint]
  - Accuracy: [Your score from checkpoint]

## Live Demo

[Link to your deployed app will go here]

## Local Setup

```bash
# Clone repository
git clone [your-repo-url]
cd AI-vs-Human-Writing-Classifier

# Install dependencies
pip install -r requirements.txt

# Run app
streamlit run app.py
```

## Tech Stack

- Streamlit
- PyTorch
- Transformers (Hugging Face)
- LIME
- Matplotlib & Seaborn
```bash
# Create virtual environment
python -m venv ai-human-classifier
source ai-human-classifier/bin/activate  # On Windows: ai-human-classifier\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install development dependencies (optional)
pip install -r requirements-dev.txt
```

### 2. Data Preparation

Place your datasets in the `data/raw/` directory. Expected format:
- CSV file with columns: `text` and `label`
- Labels: 0 for human-written, 1 for AI-generated

### 3. Configuration

Edit `config/config.yaml` to customize:
- Model parameters
- Training settings
- Data paths
- Feature extraction options

### 4. Usage Examples

```python
from src.data import TextPreprocessor, load_data, split_data
from src.models import RandomForestClassifierWrapper
from src.evaluation import ModelEvaluator

# Load and preprocess data
df = load_data('data/raw/your_dataset.csv')
train_df, val_df, test_df = split_data(df)

# Feature extraction
preprocessor = TextPreprocessor()
# ... feature extraction code

# Train model
model = RandomForestClassifierWrapper()
model.fit(X_train, y_train)

# Evaluate
evaluator = ModelEvaluator()
metrics = evaluator.evaluate_predictions(y_test, y_pred, y_proba)
```

## üîß Features

### Data Processing
- Text cleaning and normalization
- Train/validation/test splitting
- Feature extraction pipeline

### Feature Engineering
- **Linguistic Features**: Word count, sentence length, vocabulary richness
- **Stylometric Features**: Punctuation patterns, readability scores
- **Text Vectorization**: TF-IDF, Count vectorization
- **Embeddings**: Support for transformer embeddings

### Models
- **Traditional ML**: Logistic Regression, Random Forest, SVM
- **Deep Learning**: BERT-based classifiers
- **Ensemble Methods**: Voting classifiers, stacking

### Evaluation
- Cross-validation
- Multiple metrics (accuracy, precision, recall, F1, AUC)
- Confusion matrices
- Feature importance analysis
- Performance visualization

## üìä Experiments

Use Jupyter notebooks in the `notebooks/` directory:

- **Exploratory Analysis**: Data distribution, feature correlation
- **Model Comparison**: Benchmark different approaches
- **Feature Importance**: Analyze what features matter most
- **Error Analysis**: Understand model failures

## üõ†Ô∏è Development

### Running Tests
```bash
pytest tests/
```

### Code Quality
```bash
# Format code
black src/

# Check style
flake8 src/

# Sort imports
isort src/
```

### Adding New Models

1. Inherit from `BaseClassifier` in `src/models/`
2. Implement required methods: `fit()`, `predict()`, `predict_proba()`
3. Add tests in `tests/`

## üìà Results

Results are saved in the `results/` directory:
- Model performance metrics
- Confusion matrices
- Feature importance plots
- Training curves

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License.

## üôè Acknowledgments

- Hugging Face Transformers library
- scikit-learn for traditional ML models
- NLTK and spaCy for NLP utilities

## üìû Contact

Your Name - bwang0502@g.ucla.edu

Project Link: [https://github.com/bwang0502/AI-vs-Human-Writing-Classifier](https://github.com/bwang0502/AI-vs-Human-Writing-Classifier)
````
